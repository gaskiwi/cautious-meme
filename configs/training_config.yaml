# Training Configuration for RL Robotics
# TRAINING SESSION 1: Height Maximization
# Goal: Get one of the robots as high (vertically) as possible
# Note: This is the first of multiple training sessions that will be run in succession
#
# Robot Types:
#   - Type A: Bar robots with joints (bar_with_joint.urdf) - 2N robots
#   - Type B: Sphere robots (rolling_sphere.urdf) - N robots
#   - Total: 3N robots per episode
#
# Features:
#   - Random deployment on plane each episode
#   - Type A endpoints can connect to Type B spheres
#   - Joint motors configured to lift 2x body weight
#   - Reward based on highest z-level at episode end

# Environment settings
environment:
  name: "height_maximize_env"  # Training Session 1 environment
  render_mode: null  # Set to "human" for visualization
  max_episode_steps: 1000
  num_type_b_robots: 2  # N (Type A will be 2N, total 3N robots)
  spawn_radius: 3.0  # Radius for random robot deployment
  
# Agent settings
agent:
  algorithm: "PPO"  # Options: PPO, SAC, TD3, A2C
  policy: "MlpPolicy"  # Multi-layer perceptron policy
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  
# Network architecture
network:
  policy_layers: [256, 256]
  value_layers: [256, 256]
  activation: "relu"
  
# Training settings
training:
  total_timesteps: 1000000
  eval_freq: 10000
  eval_episodes: 10
  save_freq: 50000
  log_interval: 10
  
# Paths
paths:
  models: "./models"
  logs: "./logs"
  tensorboard: "./runs"
  
# AWS settings (for Spot Fleet training)
aws:
  region: "us-east-1"
  instance_type: "g4dn.xlarge"  # GPU instance for faster training
  spot_price: "0.526"  # Max price per hour
  ami_id: "ami-xxxxxxxxxx"  # Will be configured with Docker image
  s3_bucket: "rl-robotics-models"
  fleet_size: 4
  
# Monitoring
monitoring:
  use_tensorboard: true
  use_wandb: false
  wandb_project: "rl-robotics"
  wandb_entity: null
